# Wandb Hyperparameter Sweep Configuration
# Run with: wandb sweep sweep_config.yaml
# Then: wandb agent <sweep_id>

program: train.py
method: bayes  # Bayesian optimization for efficient search
metric:
  name: steg/avg_alignment
  goal: maximize

parameters:
  model:
    values:
      - qwen3-1.7b
      # Uncomment to include 4B model in sweep (requires more VRAM)
      # - qwen3-4b

  lr:
    distribution: log_uniform_values
    min: 1e-5
    max: 5e-4

  lora-r:
    values:
      - 8
      - 16
      - 32
      - 64

  lora-alpha:
    values:
      - 16
      - 32
      - 64
      - 128

  batch-size:
    values:
      - 2
      - 4
      - 8

  grad-accum:
    values:
      - 2
      - 4
      - 8

  epochs:
    values:
      - 1
      - 2
      - 3

  warmup-ratio:
    values:
      - 0.01
      - 0.03
      - 0.05
      - 0.1

command:
  - ${env}
  - python
  - ${program}
  - "--model"
  - ${model}
  - "--lr"
  - ${lr}
  - "--lora-r"
  - ${lora-r}
  - "--lora-alpha"
  - ${lora-alpha}
  - "--batch-size"
  - ${batch-size}
  - "--grad-accum"
  - ${grad-accum}
  - "--epochs"
  - ${epochs}
  - "--warmup-ratio"
  - ${warmup-ratio}
  - "--wandb-project"
  - "steg-training-sweeps"
